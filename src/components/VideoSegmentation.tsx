// src/components/VideoSegmentation.tsx
import * as React from 'react';
import {useContext, useEffect, useRef, useState} from 'react';
import {ImageSegmenter} from '@mediapipe/tasks-vision';
import {initializeImageSegmenter, predictWebcam, type PredictWebcamObject} from "@/lib/VideoSegmentationUtils";
import {BackgroundContext} from "@/components/stream-player-v2";
import {useLocalParticipant, useMediaDeviceSelect} from "@livekit/components-react";
import {createLocalAudioTrack, createLocalVideoTrack, Track} from "livekit-client";
import {ParticipantMetadata} from "@/lib/controller";


interface VideoSegmentationProps {
    videoRef: React.RefObject<HTMLVideoElement | null>;
    canvasRef: React.RefObject<HTMLCanvasElement | null>;
    isHost: boolean;
}

export const VideoSegmentation: React.FC<VideoSegmentationProps> = ({videoRef, canvasRef,isHost}) => {
    const backgroundVideoRef = useRef<HTMLVideoElement>(null);
    const imageSegmenterRef = useRef<ImageSegmenter | null>(null);
    const backgroundType = useRef<'not' | 'none' | 'blur' | 'image' | 'video'>('not');
    const [backgroundImage, setBackgroundImage] = useState<string | null>(null);
    const [backgroundVideo, setBackgroundVideo] = useState<string | null>(null);
    const backgroundImgRef = useRef<HTMLImageElement | null>(null);
    const tempCanvasRef = useRef(document.createElement('canvas'));
    const tempCtxRef = useRef<CanvasRenderingContext2D | null>(null);
    const maskCanvasRef = useRef(document.createElement('canvas'));
    const maskCtxRef = useRef<CanvasRenderingContext2D | null>(null);
    const compositeCanvasRef = useRef(document.createElement('canvas'));// å¤åˆcanvas
    const compositeCtxRef = useRef<CanvasRenderingContext2D | null>(null);// å¤åˆcanvas 2dä¸Šä¸‹æ–‡
    const backgroundVideoPlayingRef = useRef(false);
    const lastWebcamTime = useRef(-1);
    const {background,setBackground} = useContext(BackgroundContext);
    // è·å–æœ¬åœ°å‚ä¸è€…ä¿¡æ¯
    const {cameraTrack, microphoneTrack, localParticipant} = useLocalParticipant();
    const isPushTheFlowRef = useRef(false);

    // åˆå§‹åŒ–Canvasä¸Šä¸‹æ–‡å’Œæ¨¡å‹
    useEffect(() => {
        tempCtxRef.current = tempCanvasRef.current.getContext('2d');
        maskCtxRef.current = maskCanvasRef.current.getContext('2d');
        compositeCtxRef.current = compositeCanvasRef.current.getContext('2d');
        init().then(()=>{
            if (isHost) {
                toggleWebcam();
            }
        });

        return () => {
            if (videoRef.current?.srcObject) {
                (videoRef.current.srcObject as MediaStream).getTracks().forEach(track => track.stop());
            }
            if (backgroundVideoRef.current) {
                backgroundVideoRef.current.pause();
            }
            if (imageSegmenterRef.current) {
                imageSegmenterRef.current.close()
            }
        };
    }, []);

    const init = async () => {
        if (imageSegmenterRef.current) {
            imageSegmenterRef.current.close();
        }
        initializeImageSegmenter().then(imageSegmenter => {
            console.log("åˆå§‹åŒ–å›¾åƒåˆ†å‰²å™¨æˆåŠŸ");
            imageSegmenterRef.current = imageSegmenter;
        }).catch(error => {
            console.error('åˆå§‹åŒ–å›¾åƒåˆ†å‰²å™¨å¤±è´¥:', error);
        });
    };


    // å¤„ç†èƒŒæ™¯å›¾ç‰‡åŠ è½½
    useEffect(() => {
        if (backgroundImage) {
            const img = new Image();
            img.crossOrigin = 'anonymous';
            img.onload = () => {
                backgroundImgRef.current = img;
            };
            img.onerror = (e) => {
                console.error("èƒŒæ™¯å›¾ç‰‡åŠ è½½å¤±è´¥", e);
            };
            img.src = backgroundImage;
        }
    }, [backgroundImage]);

    // å¤„ç†èƒŒæ™¯è§†é¢‘åŠ è½½
    useEffect(() => {
        if (backgroundVideo && backgroundVideoRef.current) {
            backgroundVideoRef.current.src = backgroundVideo;
            backgroundVideoRef.current.loop = true;
            backgroundVideoRef.current.muted = true;

            const playVideo = async () => {
                try {
                    await backgroundVideoRef.current?.play();
                    backgroundVideoPlayingRef.current = true;
                } catch (error) {
                    console.error("èƒŒæ™¯è§†é¢‘æ’­æ”¾å¤±è´¥:", error);
                    backgroundVideoPlayingRef.current = false;
                }
            };

            playVideo();
        }
    }, [backgroundVideo]);

    // å½“æ‘„åƒå¤´è®¾å¤‡å˜åŒ–æ—¶ï¼Œæ›´æ–°æœ¬åœ°è§†é¢‘è½¨é“
    const {activeDeviceId: activeCameraDeviceId} = useMediaDeviceSelect({
        kind: "videoinput",
    });

    useEffect(() => {
        toggleWebcam()
    }, [activeCameraDeviceId]);

    const localMetadata = (localParticipant.metadata &&
        JSON.parse(localParticipant.metadata)) as ParticipantMetadata;

    useEffect(() => {
        if (localMetadata?.invited_to_stage || localMetadata?.hand_raised) {
            toggleWebcam()
        }
    }, [localMetadata?.invited_to_stage, localMetadata?.hand_raised]);

    const po = {
        imageSegmenterRef: imageSegmenterRef,
        canvasRef: canvasRef,
        videoRef: videoRef,
        backgroundType: backgroundType,
        backgroundImgRef: backgroundImgRef,
        backgroundVideoRef: backgroundVideoRef,
        backgroundVideoPlayingRef: backgroundVideoPlayingRef,
        tempCanvasRef: tempCanvasRef,
        tempCtxRef: tempCtxRef,
        maskCanvasRef: maskCanvasRef,
        maskCtxRef: maskCtxRef,
        compositeCanvasRef: compositeCanvasRef,
        compositeCtxRef: compositeCtxRef,
        lastWebcamTime: lastWebcamTime
    } as PredictWebcamObject;


    // å¼€å¯/å…³é—­æ‘„åƒå¤´
    const toggleWebcam = async () => {
        try {
            console.log("å¼€å¯æ‘„åƒå¤´");

            const track= await createLocalVideoTrack({
                deviceId: activeCameraDeviceId
            });
            const audioTrack = await createLocalAudioTrack();
            const video = videoRef.current;
            if (video) {
                track.attach(video);
                //video.srcObject = stream;
                video.onloadedmetadata = () => {
                    video.play().then(() => {
                        requestAnimationFrame(() => predictWebcam(po));
                        if (canvasRef.current) {
                            console.log("å¼€å§‹æ¨æµ")
                            const canvasStream = canvasRef.current.captureStream();
                            const videoTracks = canvasStream.getVideoTracks();
                            console.log("è§†é¢‘è½¨é“:", videoTracks);

                            // æ¨é€è§†é¢‘æµ
                            localParticipant.publishTrack(videoTracks[0],{
                                name: localParticipant.identity+'canvas',
                                stream: localParticipant.identity+'canvas',
                                source: Track.Source.Camera
                            })
                            // æ¨é€éŸ³é¢‘æµ
                            localParticipant.publishTrack(audioTrack,{
                                name: localParticipant.identity+'canvas',
                                stream: localParticipant.identity+'canvas',
                                source: Track.Source.Microphone
                            })
                        }
                        pushTheFlow()
                    }).catch(e => console.error("è§†é¢‘æ’­æ”¾å¤±è´¥:", e));
                };
            }
        } catch (error) {
            console.error('è®¿é—®æ‘„åƒå¤´å¤±è´¥:', error);
            alert('æ— æ³•è®¿é—®æ‘„åƒå¤´ï¼Œè¯·ç¡®ä¿æ‚¨å·²æˆäºˆæƒé™');
        }
    };

    const pushTheFlow = async () => {
        const tracks = localParticipant.getTracks();
        //console.log("tracks:", tracks)
        tracks.forEach(track => {
            if ((track.source===Track.Source.Camera|| track.source===Track.Source.Microphone)&&track.trackName !== localParticipant.identity + 'canvas') {
                if (track.track && track.track.mediaStreamTrack) {
                    localParticipant.unpublishTrack(track.track.mediaStreamTrack)
                }
            }

        });
        requestAnimationFrame(pushTheFlow);
    }

    // è®¾ç½®èƒŒæ™¯ç±»å‹
    const handleBackgroundTypeChange = (type: 'not' | 'none' | 'blur' | 'image' | 'video') => {
        if (!isPushTheFlowRef.current) {
            isPushTheFlowRef.current = true;
        }
        console.log("è®¾ç½®èƒŒæ™¯ç±»å‹", type);
        backgroundType.current = type;
        setBackground(type);
    };

    const fileInputRef = useRef<HTMLInputElement>(null);
    const videoFileInputRef = useRef<HTMLInputElement>(null);

    const handleImageSelect = (e: React.ChangeEvent<HTMLInputElement>) => {
        const file = e.target.files?.[0];
        if (file) {
            const reader = new FileReader();
            reader.onload = (event) => {
                if (event.target?.result) {
                    setBackgroundImage(event.target.result as string);
                    handleBackgroundTypeChange('image');
                }
            };
            reader.readAsDataURL(file);
        }
    };

    const handleVideoSelect = (e: React.ChangeEvent<HTMLInputElement>) => {
        const file = e.target.files?.[0];
        if (file) {
            const videoUrl = URL.createObjectURL(file);
            setBackgroundVideo(videoUrl);
            handleBackgroundTypeChange('video');
        }
    };

    return (
        <div className="segment-container">

            <div className="controls">

                <div className="background-controls">
                    <select
                        className="control-btn"
                        value={background}
                        onChange={(e) => {
                            const selectedValue = e.target.value as 'not' | 'none' | 'blur' | 'image' | 'video';


                            // æ ¹æ®é€‰æ‹©çš„ç±»å‹è§¦å‘ç›¸åº”çš„æ–‡ä»¶é€‰æ‹©
                            if (selectedValue === 'image') {
                                // è§¦å‘å›¾ç‰‡æ–‡ä»¶é€‰æ‹©
                                fileInputRef.current?.click();
                            } else if (selectedValue === 'video') {
                                // è§¦å‘è§†é¢‘æ–‡ä»¶é€‰æ‹©
                                videoFileInputRef.current?.click();
                            } else {
                                handleBackgroundTypeChange(selectedValue);
                            }
                        }}
                    >
                        <option value="not">ğŸ¥½ æ— èƒŒæ™¯</option>
                        <option value="none">ğŸ¨ çº¯è‰²èƒŒæ™¯</option>
                        <option value="blur">ğŸŒ€ èƒŒæ™¯æ¨¡ç³Š</option>
                        <option value="image">ğŸ–¼ï¸ å›¾ç‰‡èƒŒæ™¯</option>
                        <option value="video">ğŸ¥ è§†é¢‘èƒŒæ™¯</option>

                    </select>
                    <input
                        type="file"
                        ref={fileInputRef}
                        accept="image/*"
                        onChange={handleImageSelect}
                        style={{display: 'none'}}
                    />
                    <input
                        type="file"
                        ref={videoFileInputRef}
                        accept="video/*"
                        onChange={handleVideoSelect}
                        style={{display: 'none'}}
                    />
                </div>


            </div>


            <video ref={backgroundVideoRef} playsInline className="hidden"/>

            {/*<video ref={canvasVideoRef} playsInline*/}
            {/*       style={{*/}
            {/*           display: "block",*/}
            {/*       }}/>*/}
        </div>
    );
};

export default VideoSegmentation;
